{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of Reinforce.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to introduce you to ```Reinforce.jl``` library which is a Reinforcement Learning library written in Julia.\n",
    "\n",
    "You'll find it particularly beneficial if you're someone who is interested in\n",
    "1. Reinforcement Learning and how a real library looks like\n",
    "2. Features of Julia that make writing a library simpler\n",
    "3. Contributing to the ```Reinforce.jl``` project\n",
    "\n",
    "We'll be exploring the library in a breadth first search manner where we start from an example and understand it end to end. Then at each step we record all the abstractions we're relying on and explore them one by one until we reach the core Julia libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUT IN tree of how everything relates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's get started and run \n",
    "    \n",
    "```git clone https://github.com/JuliaML/Reinforce.jl ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there are 3 folders\n",
    "1. examples/\n",
    "2. src/\n",
    "3. test/\n",
    "\n",
    "Starting with examples/ there's just the one file called ```mountain_car.jl```\n",
    "\n",
    "Mountain car is a popular testbed for RL algorithms. In other words, if your algorithm doesn't do well on mountain car then it's unlikely to do well on harder problems like Dota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT IMAGE OF MOUNTAIN CAR HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examples/mountain_car.jl\n",
    "\n",
    "# Import the RL algorithms\n",
    "using Reinforce\n",
    "\n",
    "# Import the Mountain car environment\n",
    "using Reinforce.MountainCarEnv: MountainCar\n",
    "\n",
    "# Import the Plots plotting library and use GR as the backend \n",
    "# https://github.com/jheinen/GR.jl\n",
    "using Plots\n",
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples/mountain_car.jl\n",
    "\n",
    "# Define a deterministic policy while respect an AbstractPolicy interface\n",
    "mutable struct BasicCarPolicy <: Reinforce.AbstractPolicy end\n",
    "\n",
    "#We define our policy's actions as\n",
    "Reinforce.action(policy::BasicCarPolicy, r, s, A) = s.velocity < 0 ? 1 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MountainCar(Reinforce.MountainCarEnv.MountainCarState(0.0, 0.0), 0.0, -1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate the MountainCar environment\n",
    "env = MountainCar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the building blocks we need to start evaluating our policy \\\\(\\pi \\\\) on our environment ```env```. More so than in other languages, ```Julia``` developers often use unicode like \\\\(\\pi \\\\) so that numerical code mirrors its mathematical formulas. \n",
    "\n",
    "There is no training happenening in this example, the policy is fixed and deterministic. However, the code contains a few artifacts from a training procedure and are worth highlighting.\n",
    "\n",
    "Generally a Reinforcement Learning algorithm has access to episodes which are observations of the environment its interacting in. \n",
    "\n",
    "At any given timestep \\\\(t \\\\) our environment is in some state \\\\(s \\\\). Our agent performs an action \\\\(a \\\\) which changes the state from \\\\(s \\\\) to \\\\(s' \\\\), the state change may or may not reward the agent with a reward \\\\(s \\\\) which would be negative for punishments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -84.0, iter: 84\n"
     ]
    }
   ],
   "source": [
    "# Run next episode\n",
    "# \"!\" mark at the end of \"episode!\" means it modifies its arguments\n",
    "function episode!(env, π = RandomPolicy())\n",
    "  ep = Episode(env, π)\n",
    "  for (s, a, r, s′) in ep\n",
    "    #visualize the environment\n",
    "    gui(plot(env))\n",
    "  end\n",
    "    \n",
    "  # \"return\" keyword is optional in Julia\n",
    "  # we are choosing to only return the total reward and the total number of iterations \n",
    "  # we could return whatever metrics we want here\n",
    "  ep.total_reward, ep.niter\n",
    "end\n",
    "\n",
    "# main() function\n",
    "R, n = episode!(env, BasicCarPolicy())\n",
    "println(\"reward: $R, iter: $n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our basic example working, it's time to go one step lower in our layer of abstractions to tie up the loose ends in our understanding. So we'll look at how\n",
    "1. What does ```Reinforce.AbstractPolicy``` look like and why is it mutable?\n",
    "2. How does ```Reinforce.action``` work?\n",
    "3. What does an ```Episode``` look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce.AbstractPolicy\n",
    "\n",
    "AbstractPolicy is surpringly simple it just says that any policy needs to implement an ```action``` function that given a policy, reward, state and a set of valid actions will return the next action \\\\(a \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src/Reinforce.jl\n",
    "\n",
    "\n",
    "abstract type AbstractPolicy end\n",
    "\n",
    "\"\"\"\n",
    "    a = action(policy, r, s, A)\n",
    "\n",
    "Take in the last reward `r`, current state `s`,\n",
    "and set of valid actions `A = actions(env, s)`,\n",
    "then return the next action `a`.\n",
    "\n",
    "Note that a policy could do a 'sarsa-style' update simply by saving the last state and action `(s,a)`.\n",
    "\"\"\"\n",
    "function action end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode\n",
    "\n",
    "The Episode struct is relatively straightforward and keeps track of what you'd expect\n",
    "\n",
    "**NEED MORE DETAIL HERE** - esp on ```episodes``` data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "cannot assign a value to variable Reinforce.Episode from module Main",
     "output_type": "error",
     "traceback": [
      "cannot assign a value to variable Reinforce.Episode from module Main",
      "",
      "Stacktrace:",
      " [1] top-level scope at /Users/mark/.julia/packages/IJulia/cwvsj/src/kernel.jl:52"
     ]
    }
   ],
   "source": [
    "# src/episodes/iterators.jl\n",
    "\n",
    "mutable struct Episode{E<:AbstractEnvironment,P<:AbstractPolicy,F<:AbstractFloat}\n",
    "  env::E\n",
    "  policy::P\n",
    "  total_reward::F # total reward of the episode\n",
    "  last_reward::F\n",
    "  niter::Int      # current step in this episode\n",
    "  freq::Int       # number of steps between choosing actions\n",
    "  maxn::Int       # max steps in an episode - should be constant during an episode\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual fields are all unsurprising, we've also already seen ```AbstractPolicy``` but ```AbstractEnvironment``` is a new one so let's look at that next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AbstractEnvironment\n",
    "\n",
    "Any environment needs to subtype the ```AbstractEnvironment``` type which needs to implement the same kind of interface you'd see in the Open AI gym environment interface https://github.com/openai/gym/blob/master/gym/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxsteps"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src/Reinforce.jl\n",
    "\n",
    "abstract type AbstractEnvironment end\n",
    "\n",
    "function reset! end\n",
    "\n",
    "\"\"\"\n",
    "r, s′ = step!(env, s, a)\n",
    "Move the simulation forward, collecting a reward and getting the next state.\n",
    "\"\"\"\n",
    "function step! end\n",
    "\n",
    "finished(env::AbstractEnvironment, s′) = false\n",
    "\n",
    "\"\"\"\n",
    "A = actions(env, s)\n",
    "Return a list/set/description of valid actions from state `s`.\n",
    "\"\"\"\n",
    "function actions end\n",
    "\n",
    "state(env::AbstractEnvironment) = env.state\n",
    "\n",
    "reward(env::AbstractEnvironment) = env.reward\n",
    "\n",
    "maxsteps(env::AbstractEnvironment) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a custom environment\n",
    "\n",
    "Great at this point we understand all the main abstractions in the example tutorial. So let's now see how we can create our own environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the parameters of the MountainCar simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src/envs/mountain_car.jl\n",
    "\n",
    "const min_position = -1.2\n",
    "const max_position = 0.6\n",
    "const max_speed = 0.07\n",
    "const goal_position = 0.5\n",
    "const min_start = -0.6\n",
    "const max_start = 0.4\n",
    "\n",
    "const car_width = 0.05\n",
    "const car_height = car_width/2.0\n",
    "const clearance = 0.2*car_height\n",
    "const flag_height = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the restricted state representation we want our learning algorithm to use. There is a tradeoff here, we can feed all the parameters of the simulator to our learning algorithm but it's going to be a lot slower that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "cannot assign a value to variable MountainCarEnv.MountainCar from module Main",
     "output_type": "error",
     "traceback": [
      "cannot assign a value to variable MountainCarEnv.MountainCar from module Main",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[30]:52"
     ]
    }
   ],
   "source": [
    "mutable struct MountainCarState\n",
    "  position::Float64\n",
    "  velocity::Float64\n",
    "end\n",
    "\n",
    "mutable struct MountainCar <: AbstractEnvironment state::MountainCarState\n",
    "  reward::Float64\n",
    "  seed::Int\n",
    "end\n",
    "MountainCar(seed=-1) = MountainCar(MountainCarState(0.0, 0.0), 0.0, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to implement a simple ```reset!()``` function which will randomize the starting position and set the velocity of the car to 0. The reason we randomize the starting position is because we want to make sure our agent does well regardless of the starting position and generalize its behavior. A further improvement would be to randomize the hills the car has to climb or randomize the dimensions of the car to make sure that our agent can drive any car on any hill and not just a single car on a single hill at a single starting position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reset!(env::MountainCar)\n",
    "  if env.seed >= 0\n",
    "    seed!(env.seed)\n",
    "    env.seed = -1\n",
    "  end\n",
    "\n",
    "  env.state.position = rand(Uniform(min_start, max_start))\n",
    "  env.state.velocity = 0.0\n",
    "\n",
    "  env\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make a simplifying assumption that there are only 3 possible actions i.e: go left, go right, do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions(env::MountainCar, s) = DiscreteSet(1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a goal state where we know the agent did a good job. We again make a simplying assumption that goal states are on the right sidef of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished(env::MountainCar, s′) = env.state.position >= goal_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get to the meat of simulator with the ```step!()``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: MountainCarState not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: MountainCarState not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[38]:1"
     ]
    }
   ],
   "source": [
    "function step!(env::MountainCar, s::MountainCarState, a::Int)\n",
    "  \n",
    "  # store local variables\n",
    "  position = env.state.position\n",
    "  velocity = env.state.velocity\n",
    "\n",
    "  # update state based on action\n",
    "  velocity += (a - 2)*0.001 + cos(3*position)*(-0.0025)\n",
    "  velocity = clamp(velocity, -max_speed, max_speed)\n",
    "    \n",
    "  # update position based on change to velocity\n",
    "  position += velocity\n",
    "    \n",
    "  # don't YOLO to the left side of the screen\n",
    "  if position <= min_position && velocity < 0\n",
    "    velocity = 0\n",
    "  end\n",
    "    \n",
    "  # \n",
    "  position = clamp(position, min_position, max_position)\n",
    "  env.state = MountainCarState(position, velocity)\n",
    "  env.reward = -1\n",
    "\n",
    "  return env.reward, env.state\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "height(xs) = sin(3 * xs)*0.45 + 0.55\n",
    "rotate(xs::Array{Float64}, ys::Array{Float64}, Θ::Float64) =\n",
    "  xs.*cos(Θ) .- ys.*sin(Θ), ys.*cos(Θ) .+ xs.*sin(Θ)\n",
    "\n",
    "translate(xs::Array{Float64}, ys::Array{Float64}, t::Array{Float64}) =\n",
    "  xs .+ t[1], ys .+ t[2]\n",
    "\n",
    "@recipe function f(env::MountainCar)\n",
    "  legend := false\n",
    "  xlims := (min_position, max_position)\n",
    "  ylims := (0, 1.1)\n",
    "  grid := false\n",
    "  ticks := nothing\n",
    "\n",
    "  # Mountain\n",
    "  @series begin\n",
    "    xs = range(min_position, stop = max_position, length = 100)\n",
    "    ys = height.(xs)\n",
    "    seriestype := :path\n",
    "    linecolor --> :blue\n",
    "    xs, ys\n",
    "  end\n",
    "\n",
    "  # Car\n",
    "  @series begin\n",
    "    fillcolor --> :black\n",
    "    seriestype := :shape\n",
    "\n",
    "    θ = cos(3 * env.state.position)\n",
    "    xs = [-car_width/2, -car_width/2, car_width/2, car_width/2]\n",
    "    ys = [0, car_height, car_height, 0] .+ clearance\n",
    "    xs, ys = rotate(xs, ys, θ)\n",
    "    translate(xs, ys, [env.state.position, height(env.state.position)])\n",
    "  end\n",
    "\n",
    "  # Flag\n",
    "  @series begin\n",
    "    linecolor --> :red\n",
    "    seriestype := :path\n",
    "\n",
    "    [goal_position, goal_position], [height(goal_position), height(goal_position) + flag_height]\n",
    "  end\n",
    "end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithms - Actor critic algorithm\n",
    "\n",
    "So far we've covered how to\n",
    "1. Hardcode a policy\n",
    "2. Create a custom environment\n",
    "3. Extract actions from a policy and plug them into an environment\n",
    "4. Design data structures to make all of the above possible\n",
    "\n",
    "There's still one final loose end, an example of a reinforcement learning algorithm that can learn a robust policy. We've already covered the ```AbstractPolicy``` type so let's dive into a specific implementation of an actor critic algorithm by looking at ```src/policies/actor_critic.jl``` \n",
    "\n",
    "**Can also look at policy gradients**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "I hope this article motivates you to tinker more with Reinforcement Learning. Consider making a PR to the ```Reinforce.jl``` project - any one of the below would be extremely helpful\n",
    "\n",
    "1. More environments\n",
    "2. More policy functions\n",
    "3. YAML model configuration via ```Flux.jl```\n",
    "4. YAML scene configuration to create 3D scenes for robotics applications\n",
    "\n",
    "Also consider checking ```tests/``` in the ```Reinforce.jl``` project for more experimental code and ideas on what you could contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
